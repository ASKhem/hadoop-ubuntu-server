# ğŸ˜ Hadoop Docker Environment

> Entorno Hadoop containerizado completo para desarrollo y pruebas, listo para usar en minutos.

<div align="center">

![Hadoop Version](https://img.shields.io/badge/Hadoop-3.3.6-blue)
![Flume Version](https://img.shields.io/badge/Flume-1.11.0-green)
![Ubuntu Version](https://img.shields.io/badge/Ubuntu-22.04%20LTS-orange)
![License](https://img.shields.io/badge/license-Apache%202.0-red)
![CI/CD](https://img.shields.io/badge/CI/CD-GitHub%20Actions-blue)

</div>

## ğŸš€ CaracterÃ­sticas Principales

- âœ¨ **Hadoop 3.3.6** - Ãšltima versiÃ³n estable
- ğŸŒŠ **Apache Flume 1.11.0** - RecolecciÃ³n de datos en tiempo real
- ğŸ”„ **RotaciÃ³n automÃ¡tica de logs**
- ğŸ’¾ **Backup automÃ¡tico de HDFS**
- ğŸ“Š **MonitorizaciÃ³n de salud del sistema**
- âš¡ **GestiÃ³n automÃ¡tica de recursos**
- ğŸ”’ **Seguridad mejorada**
- ğŸ“ˆ **CI/CD integrado**

## ğŸ“‹ Requisitos Previos

| Componente | MÃ­nimo Requerido | Recomendado |
|------------|------------------|-------------|
| Docker     | 20.10+          | 23.0+       |
| RAM        | 4GB             | 8GB         |
| Disco      | 20GB            | 50GB        |
| CPU        | 2 cores         | 4 cores     |

## ğŸƒâ€â™‚ï¸ Inicio RÃ¡pido

1ï¸âƒ£ Clonar el repositorio
```bash
git clone https://github.com/ASKhem/hadoop-ubuntu-server.git
cd hadoop-ubuntu-server
```

2ï¸âƒ£ Configurar recursos (opcional)
```bash
# Editar variables en setup-hadoop.sh
MEMORY_LIMIT="4g"  # LÃ­mite de memoria
CPU_LIMIT="2"      # LÃ­mite de CPU
```

3ï¸âƒ£ Ejecutar el script de configuraciÃ³n
```bash
./setup-hadoop.sh
```

4ï¸âƒ£ Conectarse al contenedor
```bash
ssh -i id_rsa hadoopuser@localhost -p 22
```

## ğŸ”Œ Puertos y Servicios

| Puerto | Servicio                  | Healthcheck |
|--------|---------------------------|-------------|
| 22     | SSH                       | âœ…          |
| 9870   | HDFS NameNode Web UI      | âœ…          |
| 8088   | YARN ResourceManager UI   | âœ…          |
| 9000   | HDFS                      | âœ…          |
| 9864   | DataNode HTTP             | âœ…          |
| 41414  | Flume                     | âœ…          |

## ğŸ“ Estructura del Proyecto

```
hadoop-docker/
â”œâ”€â”€ ğŸ“‚ .github/
â”‚   â””â”€â”€ ğŸ“‚ workflows/        # ConfiguraciÃ³n CI/CD
â”œâ”€â”€ ğŸ“‚ conf/
â”‚   â”œâ”€â”€ ğŸ“‚ flume/           # ConfiguraciÃ³n Flume
â”‚   â”œâ”€â”€ ğŸ“‚ hadoop/          # ConfiguraciÃ³n Hadoop
â”‚   â””â”€â”€ ğŸ“‚ scripts/         # Scripts de utilidad
â”œâ”€â”€ ğŸ“„ Dockerfile           # DefiniciÃ³n multi-stage
â”œâ”€â”€ ğŸ“„ setup-hadoop.sh      # Script de configuraciÃ³n
â””â”€â”€ ğŸ“„ README.md           # DocumentaciÃ³n
```

## ğŸ› ï¸ ConfiguraciÃ³n Avanzada

### LÃ­mites de Recursos

El contenedor estÃ¡ configurado con lÃ­mites de recursos para garantizar un rendimiento Ã³ptimo:

```bash
# LÃ­mites por defecto
--memory=4g           # LÃ­mite de memoria
--memory-swap=4g      # LÃ­mite de swap
--cpus=2             # LÃ­mite de CPU
```

Para modificar estos lÃ­mites, edita las variables en `setup-hadoop.sh`:

```bash
MEMORY_LIMIT="8g"    # Aumentar memoria
CPU_LIMIT="4"        # Aumentar CPU
```

### Backup y RestauraciÃ³n

#### Realizar Backup Manual

```bash
# Ejecutar backup
/opt/hadoop/scripts/backup-hdfs.sh

# Los backups se almacenan en:
/data/backups/YYYYMMDD_HHMMSS.tar.gz
```

#### Restaurar desde Backup

```bash
# Listar backups disponibles
ls -l /data/backups/

# Restaurar desde backup especÃ­fico
/opt/hadoop/scripts/restore-hdfs.sh /data/backups/20240315_120000.tar.gz
```

### MonitorizaciÃ³n

#### Healthchecks AutomÃ¡ticos

El sistema realiza verificaciones automÃ¡ticas de salud cada 30 segundos:

- âœ… Estado de servicios HDFS
- âœ… Disponibilidad de YARN
- âœ… Conectividad SSH
- âœ… Uso de recursos

#### Logs y DiagnÃ³stico

```bash
# Ver logs de Hadoop
tail -f /opt/hadoop/logs/hadoop-*.log

# Ver logs de backup
tail -f /var/log/hadoop/backup.log

# Ver logs de restauraciÃ³n
tail -f /var/log/hadoop/restore.log
```

## ğŸ” Troubleshooting

### Problemas Comunes

<details>
<summary>El contenedor no inicia</summary>

1. Verificar recursos disponibles:
```bash
docker stats
```

2. Verificar logs del contenedor:
```bash
docker logs askhadoopx
```

3. Comprobar lÃ­mites de recursos:
```bash
cat /sys/fs/cgroup/memory/memory.limit_in_bytes
```
</details>

<details>
<summary>NameNode no estÃ¡ disponible</summary>

1. Verificar estado del servicio:
```bash
hdfs haadmin -getServiceState nn1
```

2. Revisar logs especÃ­ficos:
```bash
tail -f /opt/hadoop/logs/hadoop-hadoopuser-namenode-*.log
```

3. Reiniciar servicio:
```bash
stop-dfs.sh && start-dfs.sh
```
</details>

<details>
<summary>Problemas de rendimiento</summary>

1. Verificar uso de recursos:
```bash
htop
```

2. Comprobar configuraciÃ³n de memoria:
```bash
grep -r "heap" /opt/hadoop/etc/hadoop/
```

3. Ajustar parÃ¡metros:
```bash
# Editar hadoop-env.sh
export HADOOP_HEAPSIZE=4096
```
</details>

### Mejores PrÃ¡cticas

1. **MonitorizaciÃ³n Regular**
   - Revisar logs diariamente
   - Configurar alertas para healthchecks
   - Monitorear uso de recursos

2. **Mantenimiento**
   - Realizar backups periÃ³dicos
   - Rotar logs regularmente
   - Actualizar componentes

3. **Seguridad**
   - Cambiar contraseÃ±as regularmente
   - Mantener actualizados los certificados
   - Revisar logs de acceso

## ğŸ” Seguridad

### AutenticaciÃ³n y AutorizaciÃ³n

#### SSH y Control de Acceso
```bash
# Generar nuevo par de claves SSH
ssh-keygen -t rsa -b 4096 -f ~/.ssh/hadoop_rsa

# Configurar permisos adecuados
chmod 600 ~/.ssh/hadoop_rsa
chmod 644 ~/.ssh/hadoop_rsa.pub
```

#### Kerberos (Opcional)
```bash
# Instalar Kerberos
apt-get install krb5-kdc krb5-admin-server

# Configurar principal
kadmin.local -q "addprinc hadoop/hadoopuser@REALM"
```

### Cifrado y SSL/TLS

#### Certificados
```bash
# Generar certificado SSL
keytool -genkey -alias hadoop -keyalg RSA \
  -keystore hadoop.keystore -keysize 2048
```

#### ConfiguraciÃ³n HTTPS
```xml
<!-- ssl-server.xml -->
<property>
  <name>ssl.server.keystore.location</name>
  <value>/opt/hadoop/conf/hadoop.keystore</value>
</property>
```

### PolÃ­ticas de Seguridad

#### ACLs en HDFS
```bash
# Configurar ACLs
hdfs dfs -setfacl -m user:usuario:rw- /ruta/segura
hdfs dfs -getfacl /ruta/segura
```

#### Firewall y Puertos
```bash
# Configurar iptables
iptables -A INPUT -p tcp --dport 9000 -j ACCEPT  # HDFS
iptables -A INPUT -p tcp --dport 8088 -j ACCEPT  # YARN
iptables -P INPUT DROP  # PolÃ­tica por defecto
```

### AuditorÃ­a y Logging

#### Logs de Seguridad
```bash
# Monitorear intentos de acceso
tail -f /var/log/auth.log

# Logs de auditorÃ­a HDFS
tail -f /opt/hadoop/logs/SecurityLog.audit
```

#### MonitorizaciÃ³n de Seguridad
- ğŸ” DetecciÃ³n de accesos no autorizados
- ğŸ“Š AnÃ¡lisis de patrones de uso
- ğŸš¨ Alertas de seguridad configurables

### Mejores PrÃ¡cticas de Seguridad

1. **GestiÃ³n de Credenciales**
   - RotaciÃ³n regular de claves SSH
   - Almacenamiento seguro de contraseÃ±as
   - Uso de secretos cifrados

2. **Hardening del Sistema**
   - Actualizaciones de seguridad automÃ¡ticas
   - Principio de mÃ­nimo privilegio
   - Aislamiento de contenedores

3. **Cumplimiento y PolÃ­ticas**
   - Conformidad con GDPR/LOPD
   - PolÃ­ticas de retenciÃ³n de datos
   - Procedimientos de respuesta a incidentes

## ğŸ¤ Contribuir

1. Fork el repositorio
2. Crea tu rama (`git checkout -b feature/mejora`)
3. Commit tus cambios (`git commit -am 'AÃ±adir mejora'`)
4. Push a la rama (`git push origin feature/mejora`)
5. Abre un Pull Request

### GuÃ­a de ContribuciÃ³n

- âœ… AÃ±adir tests para nuevas funcionalidades
- âœ… Actualizar documentaciÃ³n
- âœ… Seguir estilo de cÃ³digo existente
- âœ… Mantener compatibilidad hacia atrÃ¡s

## ğŸ”§ Variables de Entorno

| Variable | DescripciÃ³n | Valor por Defecto |
|----------|-------------|-------------------|
| `HADOOP_HOME` | Directorio de instalaciÃ³n de Hadoop | `/opt/hadoop` |
| `JAVA_HOME` | Directorio de instalaciÃ³n de Java | `/usr/lib/jvm/java-11` |
| `FLUME_HOME` | Directorio de instalaciÃ³n de Flume | `/opt/flume` |
| `HADOOP_CONF_DIR` | Directorio de configuraciÃ³n | `/opt/hadoop/etc/hadoop` |
| `HADOOP_LOG_DIR` | Directorio de logs | `/opt/hadoop/logs` |

## ğŸ”— IntegraciÃ³n con Otras Herramientas

### Apache Spark
```bash
# Configurar Spark con YARN
export SPARK_HOME=/opt/spark
export HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
spark-submit --master yarn ...
```

### Apache Hive
```bash
# Configurar Hive con HDFS
export HIVE_HOME=/opt/hive
hive --service metastore
```

### Kafka Connect
```bash
# Ejemplo de conector HDFS
connect-standalone.sh connect-hdfs.properties
```

## ğŸ“Š GuÃ­a de OptimizaciÃ³n

### ConfiguraciÃ³n de Memoria
```xml
<!-- mapred-site.xml -->
<property>
  <name>mapreduce.map.memory.mb</name>
  <value>2048</value>
</property>
```

### OptimizaciÃ³n de YARN
```xml
<!-- yarn-site.xml -->
<property>
  <name>yarn.nodemanager.resource.memory-mb</name>
  <value>8192</value>
</property>
```

## ğŸ“ Ejemplos de Uso ComÃºn

### MapReduce WordCount
```bash
# Compilar y ejecutar WordCount
hadoop jar wc.jar WordCount /input /output
```

### Streaming de Datos con Flume
```bash
# Configurar agente Flume
flume-ng agent -n agent1 -c conf -f conf/flume.conf
```

### Operaciones HDFS BÃ¡sicas
```bash
# Operaciones comunes
hdfs dfs -put localfile /hdfs/path
hdfs dfs -get /hdfs/path localfile
hdfs dfs -ls /
```

## ğŸ“… Changelog

### v1.0.0 (2024-03-15)
- âœ¨ Lanzamiento inicial
- ğŸ”’ ImplementaciÃ³n de seguridad bÃ¡sica
- ğŸ“Š ConfiguraciÃ³n de monitoreo

### v1.1.0 (2024-03-20)
- ğŸš€ Mejoras en el rendimiento
- ğŸ”§ CorrecciÃ³n de bugs menores
- ğŸ“ ActualizaciÃ³n de documentaciÃ³n

## ğŸ“ Licencia

Este proyecto estÃ¡ bajo la Licencia Apache 2.0. Ver el archivo [LICENSE](LICENSE) para mÃ¡s detalles.

---

<div align="center">

**Â¿Necesitas ayuda?** [Abre un Issue](https://github.com/ASKhem/hadoop-ubuntu-server.git/issues) â€¢ [DocumentaciÃ³n](https://github.com/ASKhem/hadoop-ubuntu-server.git/wiki)

</div>
